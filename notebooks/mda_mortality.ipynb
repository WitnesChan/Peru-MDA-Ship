# %% codecell
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import math
import sklearn as skl
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
import statsmodels.api as sm
import statsmodels.formula.api as smf
import statsmodels
from patsy import dmatrices
import scipy
from scipy import stats


# %% markdown
# # Research Question 1:
# ### Is it true that a heat wave, understood as more than K days in a row with a temperature over certain high thresholds, has a effect on mortality stronger than considering the summed effect of days equally hot but not in a row?
#
# # Research Question 2:
# ### Is it possible to have a good enough estimation of the average impact of an occurring heat wave over mortality?
#
# # Research Question 3:
# ### Is there a difference in the effect size of the heat wave on  mortality between distinct groups based on age?
# %% markdown
# # Data Extraction
# %% codecell
df_temp = pd.read_csv('data/E_nasatemp_london.csv')

df_death_age_daily = pd.read_excel(
    'data/dailydeaths20052014londonfinal.xls',
    sheet_name  = 'Table 1'
    )

df_death_daily = pd.read_excel(
    'data/Dailydeathoccurrences19702014.xlsx',
    sheet_name ='Regional daily deaths',
    header = 6,  usecols = 'A:E'
    )

#%% codecell

ser_death_daily_from2000 = df_death_daily[
    (df_death_daily.Region =='E12000007') & (df_death_daily.Year>=2000)
].reset_index(drop =True)['Deaths'].dropna()

df_temp_2000to2014 = df_temp[
    df_temp.YEAR.between(2000, 2014)
    ].reset_index(drop =True)

elders_deaths = df_death_age_daily.apply(lambda r: np.sum(r[19:25]),axis =1)
# sum up death tolls with age between [5,64]
youngers_deaths = df_death_age_daily.apply(lambda r: np.sum(r[7:19]),axis =1)
# sum up death tolls with age <=4
very_young_deaths = df_death_age_daily.apply(lambda r: np.sum(r[5:7]),axis =1)


# %% markdown
# # Data visualization & preprocessing
#
# %% markdown
# # Temperature Data
# %% codecell
# Density Plot and Histogram of all arrival delays
sns.distplot(df_temp_2000to2014['TS'], hist=True, kde=True,
             bins=int(180/5), color = 'green',
             hist_kws={'edgecolor':'black'},
             kde_kws={'linewidth': 3})

H_percentiles = np.percentile(df_temp_2000to2014['TS'], [95, 97])
C_percentiles = np.percentile(df_temp_2000to2014['TS'], [5, 3])
# %% markdown
# # Mortality Data
# %% codecell
# Density Plot and Histogram of all arrival delays
sns.distplot(ser_death_daily_from2000, hist=True, kde=True,
             bins=int(180/5), color = 'red',
             hist_kws={'edgecolor':'black'},
             kde_kws={'linewidth': 3})
# %% codecell

pd.DataFrame(
    {
        'Temperature': df_temp_2000to2014['TS'],
        'Deaths': ser_death_daily_from2000
    }
).plot.scatter(x= 'Temperature', y = 'Deaths')

#%%
t = np.arange(1,len(df_temp_2000to2014)+1)

plt.figure(figsize=(40,15))
plt.plot(t, df_temp_2000to2014['TS'], 'g^', label="Temperature")
plt.plot(t, ser_death_daily_from2000, 'rx', label="Deaths")
plt.legend(fontsize=30)
plt.xlabel('days', fontsize=30)
plt.show()
# %% markdown
# ### There's a trend which suggests that the distribution of the deaths during the year has a stochasting behaviour which depends from the period of the year.
# ### So we have to take into account this fact when we will compare the deaths occurred during heatwaves with the expected deaths without heatwaves occurring.  Since this second variable depends from the period of the year.

# %% codecell
HTemp_Threshold = H_percentiles[0]
CTemp_Threshold = C_percentiles[0]
Hot_temperature_data_2000to2014 = df_temp_2000to2014[df_temp_2000to2014['TS'] >= HTemp_Threshold]
Cold_temperature_data_2000to2014 = df_temp_2000to2014[df_temp_2000to2014['TS'] <= CTemp_Threshold]
# %% codecell

hot_temperatures = Hot_temperature_data_2000to2014['TS']

a = hot_temperatures.index
boo = np.diff(a)>1
A = np.where(boo == True)
Hot_temp = np.split(a, A[0]+1)

Cold_temperatures = Cold_temperature_data_2000to2014['TS']

b = Cold_temperatures.index
boo1 = np.diff(b)>1
B = np.where(boo1 == True)
Cold_temp = np.split(b, B[0]+1)

hot_waves = []
for k in range(len(Hot_temp)):
    if len(Hot_temp[k]) >= 3:
        hot_waves.append(Hot_temp[k])

Cold_waves = []
for p in range(len(Cold_temp)):
    if len(Cold_temp[p]) >= 3:
        Cold_waves.append(Cold_temp[p])

HW = np.zeros(len(df_temp_2000to2014['TS']))
for k in range(len(hot_waves)):
    for j in range(len(hot_waves[k])):
        HW[hot_waves[k][j]] = 1

CS = np.zeros(len(df_temp_2000to2014['TS']))
for s in range(len(Cold_waves)):
    for r in range(len(Cold_waves[s])):
        CS[Cold_waves[s][r]] = 1

# %% markdown
# ### From the previous graph is possible to see an effect of the heatwaves on the number of deaths, especially in the first years from the 2000.
# ### Then it seems that no effect is anymore caused by the heatwaves on the deaths, and a possible explanation could be that the society became better organized to deal with this phenomena, maybe thanks to an effective prevention.
# ### Another, maybe even more powerful, cause could simply be that the more deaths occurred during the first heatwaves since 2000 are due not just to the heatwave characteristics but also to the average number of deaths that is decreasing with the years of medicine research.
# ### In fact we are here comparing deaths occurred during a heatwave with the average of the deaths occurred in the same period of the year but between all the 20 years considered, without taking into account the development of the society in this sense.
# ### A similar trend can be seen for the Cold Spells as well.
# ### Trend comfirmed by the loading on the Year variable which suggests that every year the number of deaths decrease in average by a 1.86% ( CI = [1.82,1.92] ) when the other variable values are the same.
# %% markdown
# # Modelling part
# %% markdown
# ### Estimate a model where the dependent variable is deaths_occurred.
# ### the independent variables should be:
# ### - day of the year.
# ### - year.
# ### - temperature of the day.
# ### - heatwave_occurring (no/yes--->0/1).
# ### - humidity and maybe other environmental factors.
# ### - noise term that stands for the deaths due to stochastic indipendent rare events.
#
# ### day of the year in order to have seasonal effects considered, year so to have a variable accounting for the medicine development in the society, temperature explained below.
#
# ### Then if the variable heatwave_occurring has a significance for the model we can say something about how their occurance affect the mortality regardless from the temperature, since we are excluding the temperature effects that are explained by that own variable.


# %%
# frame2 = {'Deaths': elders_deaths, 'Surface_Temperature': L_temperature_data_2005to2014['TS'],
#          'Year': L_temperature_data_2005to2014['YEAR'], 'Humidity': L_temperature_data_2005to2014['QV2M'] }

period = pd.date_range('2000-01-01', '2014-12-31')

modelling_df = pd.DataFrame(
    {
        'year' : period.year,
        'day_of_year': period.dayofyear,
        'Deaths': ser_death_daily_from2000,
        'Surface_Temperature': df_temp_2000to2014['TS'],
        'Year': df_temp_2000to2014['YEAR'],
        'Humidity': df_temp_2000to2014['QV2M'],
        'HeatWave':HW[len(HW)-len(modelling_df):],
        'ColdSpell': CS[len(CS)-len(modelling_df):]
    }
)

# %% codecell
modelling_df.head()

# %% markdown
# ## Outliers detection
# %% codecell
modelling_df.describe()
# %% markdown
# ### No Outliers are deleted since the extreme values of the environmental variables are the most important values that could characterize the increasing in deaths of particular days.
# %% markdown
# ## Data Scaling: (not needed)
# ### For practical reasons such as the interpretation of the intercept and an easier lecture of the coefficients meaning, we could standardize the predictors
# %% codecell
# X = np.transpose(np.array([HW, temp, humidity, day, year]))
# scaler = StandardScaler()
# X_data_scaled = scaler.fit_transform(modelling_df[['Day','HeatWave','Humidity', 'Year', 'meters2_Temperature']])
# print(X_data_scaled.mean(axis=0))
# print(X_data_scaled.std(axis=0))
# %% markdown
# ## Train/Test set division
# %% codecell
mask = np.random.rand(len(modelling_df)) < 0.8
df_train = modelling_df[mask]
df_test = modelling_df[~mask]
print('Training data set length='+str(len(df_train)))
print('Testing data set length='+str(len(df_test)))

LL = [] # loglikelihood values
models = ['Standard Poisson regression','Negative Binomial regression']
# %% markdown
# ## Deaths are poisson distributed data?
# %% markdown
# ### Elders (>64 Years old)
# %% codecell
# y = np.array(L_daily_totdeathsfrom2000)
y = np.array(elders_deaths)
sc=StandardScaler()
yy = y.reshape(-1,1)
sc.fit(yy)
y_std =sc.transform(yy)
y_std = y_std.flatten()
y_std
del yy

from scipy.stats import poisson
k = np.arange(y.max()+1)
plt.figure(figsize=(40,15))
sns.countplot(y, order=k, color='g', alpha=0.5)
mlest = y.mean()
plt.plot(k, poisson.pmf(k, mlest)*len(y), 'go', markersize=9)
print('The quality of the fit reveals that the deaths data is Poisson but with a time variable coeffient lambda')
# %% markdown
# ### Youngers (5-64 Years old)
# %% codecell
y = np.array(youngers_deaths)
sc=StandardScaler()
yy = y.reshape(-1,1)
sc.fit(yy)
y_std =sc.transform(yy)
y_std = y_std.flatten()
y_std
del yy

from scipy.stats import poisson
k = np.arange(y.max()+1)
plt.figure(figsize=(40,15))
sns.countplot(y, order=k, color='g', alpha=0.5)
mlest = y.mean()
plt.plot(k, poisson.pmf(k, mlest)*len(y), 'go', markersize=9)
# %% markdown
# ### Children (<5 Years old)
# %% codecell
y = np.array(very_young_deaths)
sc=StandardScaler()
yy = y.reshape(-1,1)
sc.fit(yy)
y_std =sc.transform(yy)
y_std = y_std.flatten()
y_std
del yy

from scipy.stats import poisson
k = np.arange(y.max()+1)
plt.figure(figsize=(40,15))
sns.countplot(y, order=k, color='g', alpha=0.5)
mlest = y.mean()
plt.plot(k, poisson.pmf(k, mlest)*len(y), 'go', markersize=9)
# %% markdown
# In the children case the data seem to follow a poisson distribution with a fixed lambda parameter, this makes sense, since
# the deaths for children are caused by random events and the progress made by the medicine in a window of years of 10 years cannot lead to visible change in the lambda parameter during the years.
#
# The same kind of reasoning old for the youg people.
#
# While for the olders the poisson distribution has more difficulties to capture the beahviour during the years of the number of deaths, and the explanation lives in the fact that probably the deaths are poisson distributed, but without a fixed parameter lambda.
# And I would link this to the fact that in curing the elders the medicine has for sure done very long steps also in a period of time as short as the one we are considering.
#
# Even more importanly was also shown that the path of deaths in the elders has
# a very visible periodicity that clearly shows the variability of the parameter lambda with respect to the time.
#
#
# %% markdown
# ## Standard Poisson regression
# %% codecell
expr = """Deaths ~ Day  + Surface_Temperature + Year + Humidity + HeatWave"""
y_train, X_train = dmatrices(expr, df_train, return_type='dataframe')
y_test, X_test = dmatrices(expr, df_test, return_type='dataframe')

# %% codecell
poisson_training_results = sm.GLM(
    y_train, X_train,
    family=sm.families.Poisson()
).fit()
LL.append(poisson_training_results.llf)
print(poisson_training_results.summary())
print(stats.chi2.ppf(0.05, 4349)) # DF = Df Residuals

print('In this case the deviance or Pearson chi2 under H0 are distributed as a chi-square and '
      'the chi-square distribution with\n DF =  Df Residuals at p-value=0.05  (95% confidence) is way smaller than both the deviance \n'
      'and the Pearson chi2, so then there is no evidence in favour of H0 which stays for a good quality fit.')

test_poisson_predictions = poisson_training_results.get_prediction(X_test)
train_poisson_predictions = poisson_training_results.get_prediction(X_train)
test_predictions_summary_frame = test_poisson_predictions.summary_frame()
train_predictions_summary_frame = train_poisson_predictions.summary_frame()
# %% codecell
fig = plt.figure(figsize=(40,15))
fig.suptitle('Residuals', fontsize=30)
plt.plot(X_train.index, np.array(train_predictions_summary_frame['mean']-np.transpose(y_train))[0], 'yo-', label='Residuals')
plt.show()
# %% codecell
predicted_counts=test_predictions_summary_frame['mean']
actual_counts = y_test['Deaths']
fig = plt.figure(figsize=(40,15))
fig.suptitle('Predicted versus actual deaths in London', fontsize=30)
predicted, = plt.plot(X_test.index, predicted_counts, 'go-', label='Predicted counts')
actual, = plt.plot(X_test.index, actual_counts, 'ro-', label='Actual counts')
plt.legend(handles=[predicted, actual], fontsize=30)
plt.show()
# %% markdown
# Is interesting to see this pattern that characterize the young people deaths, it seems that the predictions aren't able to capture the beahviour of the deaths.
#
# Some possible explanations could be that for the youngers the considered independent variables (enviromental variables) aren't able to determine the deaths for this category, deaths that seem to follow a random beahviour around the mean deaths line.
#
# This seems very reasonable since the deaths in that category aren't related to enviromental variable but more to accidental, rare, unexpected causes.


# %% markdown
# # Models allowing for over/under-dispersed data
# %% markdown
# ## Negative Binomial Regression ( AKA Poisson-Gamma Mixture )
# %% codecell
print('variance='+str(modelling_df['Deaths'].var()))
print('mean='+str(modelling_df['Deaths'].mean()))
# %% codecell
df_train['Deaths_Lambda'] = poisson_training_results.mu
df_train['Aux_OLS_Dep'] = df_train.apply(lambda x: ((x['Deaths'] - x['Deaths_Lambda'])**2 - x['Deaths_Lambda']) / x['Deaths_Lambda'],axis=1)
ols_expr = """Aux_OLS_Dep ~ Deaths_Lambda - 1"""
# %% codecell
aux_olsr_results = smf.ols(ols_expr, df_train).fit()
print(aux_olsr_results.params)
# %% markdown
# Is this value of α (0.006375) statistically significant?
#
# If it's not then the model reduce to a standard poisson regression as done before, since Var = Mean + α*mean^2 is the expression that substitute in this model the standard constraint of var = mean for a poisson model.
# %% codecell
print(stats.t.ppf(0.99, 2956),aux_olsr_results.tvalues[0])  # Test statistic for significance of α
print('Since the value of the observed statistic is bigger than the 0.99 quantile of the t distribution with\n'
      'DF = No. Observation - DF model, we can comfortably say that α is significant.\n'
      'This is due to the fact that H0: α = 0 & H1: α != 0, so there is no evidence in favour of H0 (α is not significant).')
# %% markdown
# supply the value of alpha found in STEP 2 into the statsmodels.genmod.families.family.NegativeBinomial class,
#
# and train the NB2 model on the training data set.
# %% codecell

nb2_training_results = sm.GLM(
        y_train,
        X_train,
        family=sm.families.NegativeBinomial(alpha=aux_olsr_results.params[0])
    ).fit()

# %% codecell
print(nb2_training_results.summary())
LL.append(nb2_training_results.llf)
print(stats.chi2.ppf(0.05, 4375)) # DF = Df Residuals
print('In this case the deviance or Pearson chi2 under H0 are distributed as a chi-square and '
      'the chi-square distribution with\n DF =  Df Residuals at p-value=0.05  (95% confidence) is now bigger than the deviance \n'
      'and slighty smaller than the Pearson chi2, so then there is no evidence against H0 which stays for a good quality fit.')

nb2_test_predictions = nb2_training_results.get_prediction(X_test)
nb2_train_predictions = nb2_training_results.get_prediction(X_train)
test_predictions_summary_frame_nb2 = nb2_test_predictions.summary_frame()
train_predictions_summary_frame_nb2 = nb2_train_predictions.summary_frame()
# %% codecell
predicted_counts=test_predictions_summary_frame_nb2['mean']
actual_counts = y_test['Deaths']
fig = plt.figure(figsize=(40,15))
fig.suptitle('Predicted versus actual deaths in London', fontsize=30)
predicted, = plt.plot(X_test.index, predicted_counts, 'go-', label='Predicted counts')
actual, = plt.plot(X_test.index, actual_counts, 'ro-', label='Actual counts')
plt.legend(handles=[predicted, actual], fontsize=30)
plt.show()
# %% codecell
#spr
testMSE = np.square(np.subtract(y_test['Deaths'],test_predictions_summary_frame['mean'])).mean()
testRMSE_spr = math.sqrt(testMSE)
trainMSE = np.square(np.subtract(y_train['Deaths'],train_predictions_summary_frame['mean'])).mean()
trainRMSE_spr = math.sqrt(trainMSE)
#nb2
testMSE = np.square(np.subtract(y_test['Deaths'],test_predictions_summary_frame_nb2['mean'])).mean()
testRMSE_nb2 = math.sqrt(testMSE)
trainMSE = np.square(np.subtract(y_train['Deaths'],train_predictions_summary_frame_nb2['mean'])).mean()
trainRMSE_nb2 = math.sqrt(trainMSE)
# %% markdown
# ### In every model the variable HeatWave was significant, and this suggests that, beyond the temperature, the occuring of a HeatWave is affeccting the number of deaths in a evident way.
# %% codecell
# Normalized root mean square error
IQ_test = np.quantile(y_test,0.75)-np.quantile(y_test,0.25)
IQ_train = np.quantile(y_train,0.75)-np.quantile(y_train,0.25)
NRMSE = []
NRMSE.append([trainRMSE_spr/IQ_train,testRMSE_spr/IQ_test])
NRMSE.append([trainRMSE_nb2/IQ_train,testRMSE_nb2/IQ_test])
# %% codecell
normalized_train_test_RMSE = dict(zip(models,NRMSE))
models_goodness_fit = dict(zip(models,LL))
# %% codecell
print('Log-Likelihood function:  ', models_goodness_fit)
print('Normalized [train,test] RMSE:  ', normalized_train_test_RMSE)
# %% markdown
# ## All population, London, 2000-2014
#
# ### Log-Likelihood function:
# #### {'OLS regression': 3494.474187304091, '2nd order Polynomial regression': 3751.9060928869767, 'Standard Poisson regression': -18664.77315849324, 'Negative Binomial regression': -18220.077238832433, 'Generealized Poisson regression 1': -18659.520724064732, 'Generealized Poisson regression 2': -18212.310217884973}
# ### Normalized RMSE ( [train,test] ):
# #### {'OLS regression': [nan, nan], '2nd order Polynomial regression': [5.220469103892997, 4.603345324549981], 'Standard Poisson regression': [0.5375260824251525, 0.5424715204784408], 'Negative Binomial regression': [0.535740757701683, 0.5133353377234997], 'Generealized Poisson regression 1': [0.5376428644276258, 0.5422007318319421], 'Generealized Poisson regression 2': [0.5376023977700796, 0.5425489619509641]}
#
# ## Elders (>64 years old) restricted, London, 2005-2014
#
# ### Log-Likelihood function:
# #### {'OLS regression': 1961.0612558206813, '2nd order Polynomial regression': 2110.4452240600385, 'Standard Poisson regression': -11843.249007672486, 'Negative Binomial regression': -11621.360149362907, 'Generealized Poisson regression 1': -11815.672616130216, 'Generealized Poisson regression 2': -11619.756464235228}
# ### Normalized RMSE ( [train,test] ):
# #### {'OLS regression': [nan, nan], '2nd order Polynomial regression': [5.014138747859189, 4.34479562154358], 'Standard Poisson regression': [0.6086732694664122, 0.5503384687239905], 'Negative Binomial regression': [0.6086967851089066, 0.5502795286093528], 'Generealized Poisson regression 1': [0.608673269466413, 0.5503384687238259], 'Generealized Poisson regression 2': [0.6086996559222094, 0.550273820781577]}
# %% markdown
# # Incidence Rate Ratio Interpretation (effect size of the Heat Waves)
# %% markdown
# ### For a one unit change in the predictor variable, the difference in the logs of expected counts is expected to change by the respective regression coefficient, given the other predictor variables in the model are held constant.
# ### The regression coefficient of the independent variable Heat Wave is 0.1243 in the more accurate predictive model and also in the other models tested very close to that value.
# %% markdown
# ## So coeff_Heat_Wave = 0.1238 = log(E[D2]/E[D1])
# %% markdown
# ### From that we can conclude that the expected number of Deaths given that a Heat Wave is occuring increase of 13.2 % (CI = [11.07,15.26])  with respect to the situation where all the other variables have the same values, but no Heat Wave is occurring.
# %% markdown
# # between Age-groups comparison
# %% markdown
# ## More than 64 Years old
# ### 13.5 % (CI = [10.7,16.3])
# %% markdown
# ## From 5 to 64 Years old
# ### 4.3% (CI = [0.2,8.6]) --> fit quality very poor
# %% markdown
# ## Less then 5 Years old
# ### Heat Wave variable not significant
#
# %% markdown
# ### The variable Cold Spells is not significant in describing the poisson process with variable coefficient lambda that describes the number of deaths per day.
